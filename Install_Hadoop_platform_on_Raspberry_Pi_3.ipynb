{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices\n",
    "Raspberry Pi 3 Model B `*` 1  \n",
    "8GB Micro SD card `*` 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softwares\n",
    "Latest version of [RASPBIAN STRETCH LITE](https://www.raspberrypi.org/downloads/raspbian/)   \n",
    "SD burning tool [Etcher](https://etcher.io/)  \n",
    "Apache Hadoop [hadoop-2.8.1](http://www-eu.apache.org/dist/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz)  \n",
    "Apache Hive [hive-2.3.1](http://www-eu.apache.org/dist/hive/hive-2.3.1/apache-hive-2.3.1-bin.tar.gz)  \n",
    "Apache Spark [spark-2.2.0](http://www-eu.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the Pis\n",
    "\n",
    "1. Burning the image of RASPBIAN STRETCH LITE into the Micro SD card via Etcher.  \n",
    "2. Writing ssh file and WIFI configure file into the boot directory.  \n",
    ">```shell\n",
    "touch /Volumes/boot/ssh\n",
    "```\n",
    "```shell\n",
    "vi /Volumes/boot/wpa_supplicant.conf  \n",
    "```\n",
    "```shell\n",
    "ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\n",
    "update_config=1\n",
    "network={\n",
    "        ssid=\"<network name>\"\n",
    "        psk=\"<password>\"\n",
    "}```\n",
    "\n",
    "3. Plugging the power cable into the Raspberry Pi, then finding Raspberry Pi's IP in the Route ARP table, Raspberry Pi's MAC address starts with \"B8-27-EB\", which is owned by the Raspberry Pi Foundation.\n",
    ">```\n",
    "B8-27-EB-5B-C7-9C\t192.168.1.108\n",
    "```\n",
    "4. SSH to Raspberry PI. User is pi and password is raspberry by default.  \n",
    ">```shell\n",
    "ssh pi@192.168.1.108\n",
    "```\n",
    "5. Adding public key to the Raspberry Pi.\n",
    ">```shell\n",
    "#local client\n",
    "ssh-keygen\n",
    "cat /Users/user/.ssh/id_rsa.pub\n",
    "#on Raspberry Pi, add the public key\n",
    "vi .ssh/authorized_keys\n",
    "```\n",
    "\n",
    "### Note\n",
    "By default the username to SSH in is pi and the password is raspberry. When you SSH in you'll be given the following message:\n",
    ">```\n",
    "SSH is enabled and the default password for the 'pi' user has not been changed.\n",
    "This is a security risk - please login as the 'pi' user and type 'passwd' to set a new password.\n",
    "```\n",
    "\n",
    "This will cause issues with rsync where you'll get errors like the following:  \n",
    ">```\n",
    "protocol version mismatch -- is your shell clean?\n",
    "(see the rsync man page for an explanation)\n",
    "rsync error: protocol incompatibility (code 2) at compat.c(178) [sender=3.1.2]\n",
    "```\n",
    "\n",
    "So if you don't change the default password you'll need to remove the warning script to stop that message disturbing rsync.  \n",
    ">```shell\n",
    "sudo rm /etc/profile.d/sshpwd.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing HDFS, Hive & Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scping and installing ExpressVpn: \n",
    ">```shell\n",
    "#on local client\n",
    "scp expressvpn_1.2.0_armhf.deb pi@192.168.1.108:./\n",
    "```\n",
    "```shell\n",
    "#on Raspberry Pi\n",
    "sudo dpkg -i ./expressvpn_1.2.0_armhf.deb\n",
    "#activate device\n",
    "expressvpn activate\n",
    "#connect\n",
    "expressvpn connect hk1\n",
    "```\n",
    "2. Installing prerequisite packages:\n",
    "This is the list of prerequisite packages I installed. iotop and nethogs are for telemetry and are optional. mysql-server is only used on master server and you'll save yourself some memory and CPU cycles by not installing it on slave servers. If the storage devices via the USB ports are formatted with the exfat file system which isn't supported out of the box with Raspbian so exfat-fuse and exfat-utils are needed in order to interact with them.\n",
    ">```shell\n",
    "sudo apt-get update\n",
    "sudo apt-get install \\\n",
    "    exfat-fuse \\\n",
    "    exfat-utils \\\n",
    "    iotop \\\n",
    "    mysql-server \\\n",
    "    nethogs \\\n",
    "    oracle-java8-jdk\n",
    "```\n",
    "3. Creating a user for Hive in MySQL / MariaDB.\n",
    ">```shell\n",
    "sudo su\n",
    "mysql -uroot\n",
    "```\n",
    "```SQL\n",
    "CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';\n",
    "GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost';\n",
    "FLUSH PRIVILEGES;\n",
    "exit\n",
    "```\n",
    "4. Hadoop needs to refer to other nodes in the cluster by hostname so I'll add them to the hosts file on all devices.\n",
    ">```shell\n",
    "sudo vi /etc/hosts\n",
    "```\n",
    "```\n",
    "192.168.1.108 r1\n",
    "```\n",
    "5. To ease memory pressure I'll expand the 100 MB SWAP file to 2,000 MB by changing the CONF_SWAPSIZE setting in /etc/dphys-swapfile on all devices as well.\n",
    ">```shell\n",
    "sudo vi /etc/dphys-swapfile\n",
    "```\n",
    "```\n",
    "CONF_SWAPSIZE=2000\n",
    "```\n",
    "restarting each of the devices so they'll pick up that SWAP file change\n",
    ">```shell\n",
    "sudo reboot\n",
    "```\n",
    "6. By default Hadoop uses the root account to SSH onto each of the nodes in the cluster. I'll create SSH keys to make sure this is a password-less process. On r1 I'll generate a new key pair and add it to the authorized keys of r1's root account.\n",
    ">```shell\n",
    "sudo su\n",
    "ssh-keygen\n",
    "cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n",
    "```\n",
    "**I don't have an SSH password for the root account on slave servers so I'll copy it to the authorized_keys file for the pi user on those devices.**\n",
    ">```shell\n",
    "ssh-copy-id pi@r2\n",
    "ssh-copy-id pi@r3\n",
    "exit\n",
    "```\n",
    "**Then on slave servers I'll bootstrap the .ssh folder for the root accounts on those machines and copy the authorized_keys file from the pi user's .ssh folder so the root user can accept it as well.**\n",
    ">```shell\n",
    "sudo su\n",
    "ssh-keygen\n",
    "cp /home/pi/.ssh/authorized_keys \\\n",
    "   /root/.ssh/authorized_keys\n",
    "exit\n",
    "```\n",
    "7. There are settings that will be used by HDFS, Hive and Spark and by both the root and the pi user accounts. To centralise these settings I've stored them in /etc/profile and created a symbolic link from /root/.bashrc to this file as well. That way all users will have the same settings and they can be centrally managed on each device.\n",
    ">```shell\n",
    "sudo vi /etc/profile\n",
    "```\n",
    "```shell\n",
    "#add below:\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:/opt/hive/bin:/opt/spark/bin\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
    "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"\n",
    "export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\n",
    "export SPARK_HOME=/opt/spark\n",
    "export SPARK_CONF_DIR=/opt/spark/conf\n",
    "export SPARK_MASTER_HOST=r1\n",
    "export JAVA_HOME=/usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/jre\n",
    "```\n",
    "```shell\n",
    "sudo ln -sf /etc/profile /root/.bashrc\n",
    "source /etc/profile\n",
    "```\n",
    "On r1 I'll create the folders used by the various Hadoop tools used in this benchmark.\n",
    ">```shell\n",
    "sudo mkdir -p /opt/{hadoop,hdfs/{datanode,namenode},hive,spark}\n",
    "```\n",
    "**On all servers, the USB connecting storage is represented by /dev/sda1. I'll mount it to /mnt/usb.**\n",
    ">```shell\n",
    "sudo mkdir -p /mnt/usb\n",
    "sudo mount /dev/sda1 /mnt/usb\n",
    "```\n",
    "**Creating the application folders and the two data node folders HDFS will use for heterogeneous storage.**\n",
    "```shell\n",
    ">sudo mkdir -p /opt/{hadoop,hdfs/datanode,spark},/mnt/usb/hdfs/datanode\n",
    "```\n",
    "8. SCPing the sorftware packages from client to Raspberry Pi master server\n",
    ">```shell\n",
    "#on the client\n",
    "scp spark-2.2.0-bin-hadoop2.7.tgz pi@192.168.1.108:./\n",
    "scp hadoop-2.8.1.tar.gz pi@192.168.1.108:./\n",
    "scp apache-hive-2.3.1-bin.tar.gz pi@192.168.1.108:./\n",
    "```\n",
    "or directly downloading to the Raspberry Pi\n",
    ">```shell\n",
    "DIST=http://www-eu.apache.org/dist\n",
    "wget -c -O hadoop.tar.gz $DIST/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz\n",
    "wget -c -O hive.tar.gz   $DIST/hive/hive-2.3.0/apache-hive-2.3.0-bin.tar.gz\n",
    "wget -c -O spark.tgz     DIST/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\n",
    "```\n",
    "Hadoop is 405 MB in size when compressed, Hive is 221 MB and Spark is 194 MB. Hadoop expands to 2.1 GB but 1.9 GB of that is documentation so I'll exclude the docs from the extraction.\n",
    ">```shell\n",
    "sudo tar xvf hadoop-2.8.1.tar.gz \\\n",
    "    --directory=/opt/hadoop \\\n",
    "    --exclude=hadoop-2.8.1/share/doc \\\n",
    "    --strip 1\n",
    "```\n",
    "Hive is 172 MB decompressed but 102 MB of that is unit tests so I'll exclude those from extraction.\n",
    ">```shell\n",
    "sudo tar xvf apache-hive-2.3.1-bin.tar.gz \\\n",
    "    --directory=/opt/hive \\\n",
    "    --exclude=apache-hive-2.3.0-bin/ql/src/test \\\n",
    "    --strip 1\n",
    "```\n",
    "The following will extract Spark to it's installation folder.\n",
    ">```shell\n",
    "sudo tar xzvf spark-2.2.0-bin-hadoop2.7.tgz \\\n",
    "  --directory=/opt/spark \\\n",
    "  --strip 1\n",
    "```\n",
    "**I'll specify the master and slaves for the HDFS cluster. r1 will serve as both a master and a slave so that all the Raspberry Pis will be busy when processing workloads.**\n",
    ">```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/master\n",
    "```\n",
    "```\n",
    "r1\n",
    "```\n",
    "```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/slaves\n",
    "```\n",
    "```\n",
    "r1\n",
    "r2\n",
    "r3\n",
    "```\n",
    "**I'll then create two files with configuration overrides needed for this HDFS cluster. I'll be setting a default replication factor of 3 for all the files stored on HDFS so that they're copied onto each machine in full. There are multiple storage folders used on r2 and r3 and to avoid filling the Micro SD card used by both HDFS and the OS I've set a limit of 3 GB that must be available before HDFS writes any blocks to a partition.**\n",
    ">```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/core-site.xml\n",
    "```\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.default.name</name>\n",
    "        <value>hdfs://r1:9000/</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.default.FS</name>\n",
    "        <value>hdfs://r1:9000/</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/hdfs-site.xml\n",
    "```\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.datanode.data.dir</name>\n",
    "        <value>/opt/hdfs/datanode</value>\n",
    "        <final>true</final>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.namenode.name.dir</name>\n",
    "        <value>/opt/hdfs/namenode</value>\n",
    "        <final>true</final>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.namenode.http-address</name>\n",
    "        <value>r1:50070</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>3</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.datanode.du.reserved</name>\n",
    "        <value>3221225472</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "9. **Syncing Hadoop's binaries and configuration onto the slave servers.**\n",
    ">```shell\n",
    "for SERVER in r2 r3\n",
    "do\n",
    "    sudo rsync --archive \\\n",
    "               --one-file-system \\\n",
    "               --partial \\\n",
    "               --progress \\\n",
    "               --compress \\\n",
    "               /opt/hadoop/ SERVER:/opt/hadoop/\n",
    "done\n",
    "```\n",
    "On r2 and r3 I'll adjust the HDFS configuration to include both storage folders.\n",
    ">```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/hdfs-site.xml\n",
    "```\n",
    "```xml\n",
    "<property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/mnt/usb/hdfs/datanode,/opt/hdfs/datanode</value>\n",
    "    <final>true</final>\n",
    "</property>\n",
    "```\n",
    "10. At this point I'll need to load an interactive root shell in order to run three commands.\n",
    ">```shell\n",
    "sudo su\n",
    "```\n",
    "The first command will format the HDFS name node.\n",
    ">```shell\n",
    "hdfs namenode -format\n",
    "```\n",
    "The next will launch HDFS across the whole cluster. This command will SSH as the root user into each device.\n",
    ">```shell\n",
    "start-dfs.sh\n",
    "#stop-dfs.sh\n",
    "```\n",
    "The third command sets permissive access for the pi user on HDFS.\n",
    ">```shell\n",
    "hdfs dfs -chown pi /\n",
    "```\n",
    "11. Once that's all done I can check the capacity available across the cluster. The first line of output is the aggregate of each of the devices. The remaining lines are the amount of capacity on each respective device.\n",
    ">```shell\n",
    "hdfs dfsadmin -report | grep 'Configured Capacity'\n",
    "```\n",
    "```\n",
    "Configured Capacity: 314337058816 (292.75 GB)\n",
    "Configured Capacity: 125850886144 (117.21 GB)\n",
    "Configured Capacity: 94243086336 (87.77 GB)\n",
    "Configured Capacity: 94243086336 (87.77 GB)\n",
    "```\n",
    "```shell\n",
    "exit\n",
    "```\n",
    "12. The following will configure Hive to use MySQL / MariaDB to store it's metadata. This only needs to happen on master server.\n",
    ">```shell\n",
    "sudo vi /opt/hive/conf/hive-site.xml\n",
    "```\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionURL</name>\n",
    "        <value>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionDriverName</name>\n",
    "        <value>com.mysql.jdbc.Driver</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionUserName</name>\n",
    "        <value>hive</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionPassword</name>\n",
    "        <value>hive</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>datanucleus.autoCreateSchema</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>datanucleus.fixedDatastore</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>datanucleus.autoCreateTables</name>\n",
    "        <value>True</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "Downloading the MySQL / MariaDB connector for Hive to use.\n",
    ">```shell\n",
    "sudo wget -c http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.28/mysql-connector-java-5.1.28.jar \\\n",
    "    -P /opt/hive/lib/\n",
    "```\n",
    "Initialising the schema and launch the Hive Metastore.\n",
    ">```shell\n",
    "sudo su\n",
    "schematool -dbType mysql -initSchema\n",
    "```\n",
    "```shell\n",
    "hive --service metastore &\n",
    "```\n",
    "Spark will need to know of Hive's configuration settings so I'll link the configuration file into Spark's configuration folder.\n",
    ">```shell\n",
    "sudo ln -s /opt/hive/conf/hive-site.xml /opt/spark/conf/hive-site.xml\n",
    "```\n",
    "Spark too will also need to use the same MySQL / MariaDB connector.\n",
    ">```shell\n",
    "sudo ln -s /opt/hive/lib/mysql-connector-java-5.1.28.jar \\\n",
    "             /opt/spark/jars/mysql-connector-java-5.1.28.jar\n",
    "```\n",
    "13. When you launch pyspark, spark-submit or spark-sql the Spark libraries from the master node are copied onto HDFS and shared amongst the worker nodes. Reading 200 MB off of the Micro SD card every time one of these applications launches adds a lot of delay so I'll package up these libraries, upload them to HDFS and in the Spark configuration I'll make sure the cached jar of libraries is used instead.\n",
    ">```shell\n",
    "jar cv0f ~/spark-libs.jar -C /opt/spark/jars/ .\n",
    "hdfs dfs -mkdir /spark-libs\n",
    "hdfs dfs -put ~/spark-libs.jar /spark-libs/\n",
    "sudo vi /opt/spark/conf/spark-defaults.conf\n",
    "```\n",
    "```\n",
    "spark.master spark://r1:7077\n",
    "spark.yarn.preserve.staging.files true\n",
    "spark.yarn.archive hdfs:///spark-libs/spark-libs.jar\n",
    "```\n",
    "I found a 650 MB memory limit on the various Spark components allowed everything to work without complaining.\n",
    ">```shell\n",
    "sudo vi /opt/spark/conf/spark-env.sh\n",
    "```\n",
    "```\n",
    "SPARK_EXECUTOR_MEMORY=650m\n",
    "SPARK_DRIVER_MEMORY=650m\n",
    "SPARK_WORKER_MEMORY=650m\n",
    "SPARK_DAEMON_MEMORY=650m\n",
    "```\n",
    "**Spark jobs will run on all Raspberry Pis.**\n",
    ">```shell\n",
    "sudo vi /opt/spark/conf/slaves\n",
    "```\n",
    "```\n",
    "r1\n",
    "r2\n",
    "r3\n",
    "```\n",
    "**With that done I'll distribute Spark and its configuration to the other nodes.**\n",
    ">```shell\n",
    "for SERVER in r2 r3\n",
    "do\n",
    "    sudo rsync --archive \\\n",
    "               --one-file-system \\\n",
    "               --partial \\\n",
    "               --progress \\\n",
    "               --compress \\\n",
    "               --exclude /opt/spark/logs \\\n",
    "               /opt/spark/ SERVER:/opt/spark/\n",
    "done\n",
    "```\n",
    "14. To save memory I didn't launch Spark until after I have populated all the data onto HDFS but it makes sense to mention the launch commands here. They are as follows:\n",
    ">```shell\n",
    "sudo /opt/spark/sbin/start-master.sh\n",
    "#sudo /opt/spark/sbin/stop-master.sh\n",
    "sudo /opt/spark/sbin/start-slaves.sh\n",
    "#sudo /opt/spark/sbin/stop-slaves.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Issue\n",
    "Failing to spark-sql\n",
    ">```shell\n",
    "spark-sql \\\n",
    "    --master spark://r1:7077 \\\n",
    "    --num-executors 1\n",
    "```\n",
    "*Failed to connect to master r1:7077*\n",
    "check the port status:\n",
    "```shell\n",
    "ssh -v -p 7077 r1\n",
    "```\n",
    "*OpenSSH_7.4p1 Raspbian-10+deb9u1, OpenSSL 1.0.2l  25 May 2017  \n",
    "debug1: Reading configuration data /etc/ssh/ssh_config  \n",
    "debug1: /etc/ssh/ssh_config line 19: Applying options for **  \n",
    "debug1: Connecting to r1 [192.168.1.108] port 7077.  \n",
    "debug1: connect to address 192.168.1.108 port 7077: Connection refused  \n",
    "ssh: connect to host r1 port 7077: Connection refused*  \n",
    "check port status:\n",
    "```shell\n",
    "netstat -pln\n",
    "```\n",
    "127.0.0.1:7077 is open, but 192.168.1.108:7077 is not open. \n",
    "redo this step, I just place r1 before, suppose the mechanism is: if there is only master server as slave server, Spark will only listen on 127.0.0.1:7077 for local scope. If there is other remote slave server in this configure file, than Spark will listern on 192.168.1.108:7077 for cluster scope.\n",
    "```shell\n",
    "sudo vi /opt/spark/conf/slaves\n",
    "r1\n",
    "r2\n",
    "r3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
