{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices\n",
    "Raspberry Pi 3 Model B `*` 1  \n",
    "8GB Micro SD card `*` 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softwares\n",
    "Latest version of [RASPBIAN STRETCH LITE](https://www.raspberrypi.org/downloads/raspbian/)   \n",
    "SD burning tool [Etcher](https://etcher.io/)  \n",
    "Apache Hadoop [hadoop-2.8.1](http://www-eu.apache.org/dist/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz)  \n",
    "Apache Hive [hive-2.3.1](http://www-eu.apache.org/dist/hive/hive-2.3.1/apache-hive-2.3.1-bin.tar.gz)  \n",
    "Apache Spark [spark-2.2.0](http://www-eu.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the Pis\n",
    "\n",
    "1. Burning the image of RASPBIAN STRETCH LITE into the Micro SD card via Etcher.  \n",
    "2. Writing ssh file and WIFI configure file into the boot directory.  \n",
    ">```shell\n",
    "touch /Volumes/boot/ssh\n",
    "```\n",
    "```shell\n",
    "vi /Volumes/boot/wpa_supplicant.conf  \n",
    "```\n",
    "```shell\n",
    "ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\n",
    "update_config=1\n",
    "network={\n",
    "        ssid=\"<network name>\"\n",
    "        psk=\"<password>\"\n",
    "}```\n",
    "\n",
    "3. Plugging the power cable into the Raspberry Pi, then finding Raspberry Pi's IP in the Route ARP table, Raspberry Pi's MAC address starts with \"B8-27-EB\", which is owned by the Raspberry Pi Foundation.\n",
    ">```\n",
    "B8-27-EB-5B-C7-9C\t192.168.1.108\n",
    "```\n",
    "4. SSH to Raspberry PI. User is pi and password is raspberry by default.  \n",
    ">```shell\n",
    "ssh pi@192.168.1.108\n",
    "```\n",
    "5. Adding public key to the Raspberry Pi.\n",
    ">```shell\n",
    "#local client\n",
    "ssh-keygen\n",
    "cat /Users/user/.ssh/id_rsa.pub\n",
    "#on Raspberry Pi, add the public key\n",
    "vi .ssh/authorized_keys\n",
    "```\n",
    "\n",
    "### Note\n",
    "By default the username to SSH in is pi and the password is raspberry. When you SSH in you'll be given the following message:\n",
    ">```\n",
    "SSH is enabled and the default password for the 'pi' user has not been changed.\n",
    "This is a security risk - please login as the 'pi' user and type 'passwd' to set a new password.\n",
    "```\n",
    "\n",
    "This will cause issues with rsync where you'll get errors like the following:  \n",
    ">```\n",
    "protocol version mismatch -- is your shell clean?\n",
    "(see the rsync man page for an explanation)\n",
    "rsync error: protocol incompatibility (code 2) at compat.c(178) [sender=3.1.2]\n",
    "```\n",
    "\n",
    "So if you don't change the default password you'll need to remove the warning script to stop that message disturbing rsync.  \n",
    ">```shell\n",
    "sudo rm /etc/profile.d/sshpwd.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing HDFS, Hive & Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scping and installing ExpressVpn: \n",
    ">```shell\n",
    "#on local client\n",
    "scp expressvpn_1.2.0_armhf.deb pi@192.168.1.108:./\n",
    "```\n",
    "```shell\n",
    "#on Raspberry Pi\n",
    "sudo dpkg -i ./expressvpn_1.2.0_armhf.deb\n",
    "#activate device\n",
    "expressvpn activate\n",
    "#connect\n",
    "expressvpn connect hk1\n",
    "```\n",
    "2. Installing prerequisite packages:\n",
    "This is the list of prerequisite packages I installed. iotop and nethogs are for telemetry and are optional. mysql-server is only used on master server and you'll save yourself some memory and CPU cycles by not installing it on slave servers. If the storage devices via the USB ports are formatted with the exfat file system which isn't supported out of the box with Raspbian so exfat-fuse and exfat-utils are needed in order to interact with them.\n",
    ">```shell\n",
    "sudo apt-get update\n",
    "sudo apt-get install \\\n",
    "    exfat-fuse \\\n",
    "    exfat-utils \\\n",
    "    iotop \\\n",
    "    mysql-server \\\n",
    "    nethogs \\\n",
    "    oracle-java8-jdk\n",
    "```\n",
    "3. Creating a user for Hive in MySQL / MariaDB.\n",
    ">```shell\n",
    "sudo su\n",
    "mysql -uroot\n",
    "```\n",
    "```SQL\n",
    "CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';\n",
    "GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost';\n",
    "FLUSH PRIVILEGES;\n",
    "exit\n",
    "```\n",
    "4. Hadoop needs to refer to other nodes in the cluster by hostname so I'll add them to the hosts file on all devices.\n",
    ">```shell\n",
    "sudo vi /etc/hosts\n",
    "```\n",
    "```\n",
    "192.168.1.108 r1\n",
    "```\n",
    "5. To ease memory pressure I'll expand the 100 MB SWAP file to 2,000 MB by changing the CONF_SWAPSIZE setting in /etc/dphys-swapfile on all devices as well.\n",
    ">```shell\n",
    "sudo vi /etc/dphys-swapfile\n",
    "```\n",
    "```\n",
    "CONF_SWAPSIZE=2000\n",
    "```\n",
    "restarting each of the devices so they'll pick up that SWAP file change\n",
    ">```shell\n",
    "sudo reboot\n",
    "```\n",
    "6. By default Hadoop uses the root account to SSH onto each of the nodes in the cluster. I'll create SSH keys to make sure this is a password-less process. On r1 I'll generate a new key pair and add it to the authorized keys of r1's root account.\n",
    ">```shell\n",
    "sudo su\n",
    "ssh-keygen\n",
    "cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n",
    "```\n",
    "**I don't have an SSH password for the root account on slave servers so I'll copy it to the authorized_keys file for the pi user on those devices.**\n",
    ">```shell\n",
    "ssh-copy-id pi@r2\n",
    "ssh-copy-id pi@r3\n",
    "exit\n",
    "```\n",
    "**Then on slave servers I'll bootstrap the .ssh folder for the root accounts on those machines and copy the authorized_keys file from the pi user's .ssh folder so the root user can accept it as well.**\n",
    ">```shell\n",
    "sudo su\n",
    "ssh-keygen\n",
    "cp /home/pi/.ssh/authorized_keys \\\n",
    "   /root/.ssh/authorized_keys\n",
    "exit\n",
    "```\n",
    "7. There are settings that will be used by HDFS, Hive and Spark and by both the root and the pi user accounts. To centralise these settings I've stored them in /etc/profile and created a symbolic link from /root/.bashrc to this file as well. That way all users will have the same settings and they can be centrally managed on each device.\n",
    ">```shell\n",
    "sudo vi /etc/profile\n",
    "```\n",
    "```shell\n",
    "#add below:\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:/opt/hive/bin:/opt/spark/bin:/opt/pig/bin\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
    "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"\n",
    "export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\n",
    "export SPARK_HOME=/opt/spark\n",
    "export SPARK_CONF_DIR=/opt/spark/conf\n",
    "export SPARK_MASTER_HOST=r1\n",
    "export JAVA_HOME=/usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/jre\n",
    "export ZOO_HOME=/opt/zookeeper\n",
    "export KAFKA_HOME=/opt/kafka\n",
    "export PIG_HOME=/opt/pig\n",
    "export HBASE_HOME=/opt/hbase\n",
    "export PIG_CLASSPATH=/opt/hadoop/etc/hadoop\n",
    "```\n",
    "```shell\n",
    "sudo ln -sf /etc/profile /root/.bashrc\n",
    "source /etc/profile\n",
    "```\n",
    "On r1 I'll create the folders used by the various Hadoop tools used in this benchmark.\n",
    ">```shell\n",
    "sudo mkdir -p /opt/{hadoop,hdfs/{datanode,namenode},hive,spark,kafka}\n",
    "```\n",
    "**On all servers, the USB connecting storage is represented by /dev/sda1. I'll mount it to /mnt/usb.**\n",
    ">```shell\n",
    "sudo mkdir -p /mnt/usb\n",
    "sudo mount /dev/sda1 /mnt/usb\n",
    "```\n",
    "**Creating the application folders and the two data node folders HDFS will use for heterogeneous storage.**\n",
    "```shell\n",
    ">sudo mkdir -p /opt/{hadoop,hdfs/datanode,spark,kafka},/mnt/usb/hdfs/datanode\n",
    "```\n",
    "8. SCPing the sorftware packages from client to Raspberry Pi master server\n",
    ">```shell\n",
    "#on the client\n",
    "scp spark-2.2.0-bin-hadoop2.7.tgz pi@192.168.1.108:./\n",
    "scp hadoop-2.8.1.tar.gz pi@192.168.1.108:./\n",
    "scp apache-hive-2.3.1-bin.tar.gz pi@192.168.1.108:./\n",
    "```\n",
    "or directly downloading to the Raspberry Pi\n",
    ">```shell\n",
    "DIST=http://www-eu.apache.org/dist\n",
    "wget -c -O hadoop.tar.gz $DIST/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz\n",
    "wget -c -O hive.tar.gz   $DIST/hive/hive-2.3.0/apache-hive-2.3.0-bin.tar.gz\n",
    "wget -c -O spark.tgz     DIST/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\n",
    "```\n",
    "Hadoop is 405 MB in size when compressed, Hive is 221 MB and Spark is 194 MB. Hadoop expands to 2.1 GB but 1.9 GB of that is documentation so I'll exclude the docs from the extraction.\n",
    ">```shell\n",
    "sudo tar xvf hadoop-2.8.1.tar.gz \\\n",
    "    --directory=/opt/hadoop \\\n",
    "    --exclude=hadoop-2.8.1/share/doc \\\n",
    "    --strip 1\n",
    "```\n",
    "Hive is 172 MB decompressed but 102 MB of that is unit tests so I'll exclude those from extraction.\n",
    ">```shell\n",
    "sudo tar xvf apache-hive-2.3.1-bin.tar.gz \\\n",
    "    --directory=/opt/hive \\\n",
    "    --exclude=apache-hive-2.3.0-bin/ql/src/test \\\n",
    "    --strip 1\n",
    "```\n",
    "The following will extract Spark to it's installation folder.\n",
    ">```shell\n",
    "sudo tar xzvf spark-2.2.0-bin-hadoop2.7.tgz \\\n",
    "  --directory=/opt/spark \\\n",
    "  --strip 1\n",
    "```\n",
    "**I'll specify the master and slaves for the HDFS cluster. r1 will serve as both a master and a slave so that all the Raspberry Pis will be busy when processing workloads.**\n",
    ">```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/master\n",
    "```\n",
    "```\n",
    "r1\n",
    "```\n",
    "```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/slaves\n",
    "```\n",
    "```\n",
    "r1\n",
    "r2\n",
    "r3\n",
    "```\n",
    "**I'll then create two files with configuration overrides needed for this HDFS cluster. I'll be setting a default replication factor of 3 for all the files stored on HDFS so that they're copied onto each machine in full. There are multiple storage folders used on r2 and r3 and to avoid filling the Micro SD card used by both HDFS and the OS I've set a limit of 3 GB that must be available before HDFS writes any blocks to a partition.**\n",
    ">```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/core-site.xml\n",
    "```\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.default.name</name>\n",
    "        <value>hdfs://r1:9000/</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.default.FS</name>\n",
    "        <value>hdfs://r1:9000/</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/hdfs-site.xml\n",
    "```\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.datanode.data.dir</name>\n",
    "        <value>/opt/hdfs/datanode</value>\n",
    "        <final>true</final>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.namenode.name.dir</name>\n",
    "        <value>/opt/hdfs/namenode</value>\n",
    "        <final>true</final>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.namenode.http-address</name>\n",
    "        <value>r1:50070</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>3</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.datanode.du.reserved</name>\n",
    "        <value>3221225472</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "9. **Syncing Hadoop's binaries and configuration onto the slave servers.**\n",
    ">```shell\n",
    "for SERVER in r2 r3\n",
    "do\n",
    "    sudo rsync --archive \\\n",
    "               --one-file-system \\\n",
    "               --partial \\\n",
    "               --progress \\\n",
    "               --compress \\\n",
    "               /opt/hadoop/ SERVER:/opt/hadoop/\n",
    "done\n",
    "```\n",
    "On r2 and r3 I'll adjust the HDFS configuration to include both storage folders.\n",
    ">```shell\n",
    "sudo vi /opt/hadoop/etc/hadoop/hdfs-site.xml\n",
    "```\n",
    "```xml\n",
    "<property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/mnt/usb/hdfs/datanode,/opt/hdfs/datanode</value>\n",
    "    <final>true</final>\n",
    "</property>\n",
    "```\n",
    "10. At this point I'll need to load an interactive root shell in order to run three commands.\n",
    ">```shell\n",
    "sudo su\n",
    "```\n",
    "The first command will format the HDFS name node.\n",
    ">```shell\n",
    "hdfs namenode -format\n",
    "```\n",
    "The next will launch HDFS across the whole cluster. This command will SSH as the root user into each device.\n",
    ">```shell\n",
    "start-dfs.sh\n",
    "#stop-dfs.sh\n",
    "```\n",
    "The third command sets permissive access for the pi user on HDFS.\n",
    ">```shell\n",
    "hdfs dfs -chown pi /\n",
    "```\n",
    "11. Once that's all done I can check the capacity available across the cluster. The first line of output is the aggregate of each of the devices. The remaining lines are the amount of capacity on each respective device.\n",
    ">```shell\n",
    "hdfs dfsadmin -report | grep 'Configured Capacity'\n",
    "```\n",
    "```\n",
    "Configured Capacity: 314337058816 (292.75 GB)\n",
    "Configured Capacity: 125850886144 (117.21 GB)\n",
    "Configured Capacity: 94243086336 (87.77 GB)\n",
    "Configured Capacity: 94243086336 (87.77 GB)\n",
    "```\n",
    "```shell\n",
    "exit\n",
    "```\n",
    "12. The following will configure Hive to use MySQL / MariaDB to store it's metadata. This only needs to happen on master server.\n",
    ">```shell\n",
    "sudo vi /opt/hive/conf/hive-site.xml\n",
    "```\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionURL</name>\n",
    "        <value>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionDriverName</name>\n",
    "        <value>com.mysql.jdbc.Driver</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionUserName</name>\n",
    "        <value>hive</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionPassword</name>\n",
    "        <value>hive</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>datanucleus.autoCreateSchema</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>datanucleus.fixedDatastore</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>datanucleus.autoCreateTables</name>\n",
    "        <value>True</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "Downloading the MySQL / MariaDB connector for Hive to use.\n",
    ">```shell\n",
    "sudo wget -c http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.28/mysql-connector-java-5.1.28.jar \\\n",
    "    -P /opt/hive/lib/\n",
    "```\n",
    "Initialising the schema and launch the Hive Metastore.\n",
    ">```shell\n",
    "sudo su\n",
    "schematool -dbType mysql -initSchema\n",
    "```\n",
    "```shell\n",
    "hive --service metastore &\n",
    "```\n",
    "Spark will need to know of Hive's configuration settings so I'll link the configuration file into Spark's configuration folder.\n",
    ">```shell\n",
    "sudo ln -s /opt/hive/conf/hive-site.xml /opt/spark/conf/hive-site.xml\n",
    "```\n",
    "Spark too will also need to use the same MySQL / MariaDB connector.\n",
    ">```shell\n",
    "sudo ln -s /opt/hive/lib/mysql-connector-java-5.1.28.jar \\\n",
    "             /opt/spark/jars/mysql-connector-java-5.1.28.jar\n",
    "```\n",
    "13. When you launch pyspark, spark-submit or spark-sql the Spark libraries from the master node are copied onto HDFS and shared amongst the worker nodes. Reading 200 MB off of the Micro SD card every time one of these applications launches adds a lot of delay so I'll package up these libraries, upload them to HDFS and in the Spark configuration I'll make sure the cached jar of libraries is used instead.\n",
    ">```shell\n",
    "jar cv0f ~/spark-libs.jar -C /opt/spark/jars/ .\n",
    "hdfs dfs -mkdir /spark-libs\n",
    "hdfs dfs -put ~/spark-libs.jar /spark-libs/\n",
    "sudo vi /opt/spark/conf/spark-defaults.conf\n",
    "```\n",
    "```\n",
    "spark.master spark://r1:7077\n",
    "spark.yarn.preserve.staging.files true\n",
    "spark.yarn.archive hdfs:///spark-libs/spark-libs.jar\n",
    "```\n",
    "I found a 650 MB memory limit on the various Spark components allowed everything to work without complaining.\n",
    ">```shell\n",
    "sudo vi /opt/spark/conf/spark-env.sh\n",
    "```\n",
    "```\n",
    "SPARK_EXECUTOR_MEMORY=650m\n",
    "SPARK_DRIVER_MEMORY=650m\n",
    "SPARK_WORKER_MEMORY=650m\n",
    "SPARK_DAEMON_MEMORY=650m\n",
    "```\n",
    "**Spark jobs will run on all Raspberry Pis.**\n",
    ">```shell\n",
    "sudo vi /opt/spark/conf/slaves\n",
    "```\n",
    "```\n",
    "r1\n",
    "r2\n",
    "r3\n",
    "```\n",
    "**With that done I'll distribute Spark and its configuration to the other nodes.**\n",
    ">```shell\n",
    "for SERVER in r2 r3\n",
    "do\n",
    "    sudo rsync --archive \\\n",
    "               --one-file-system \\\n",
    "               --partial \\\n",
    "               --progress \\\n",
    "               --compress \\\n",
    "               --exclude /opt/spark/logs \\\n",
    "               /opt/spark/ SERVER:/opt/spark/\n",
    "done\n",
    "```\n",
    "14. To save memory I didn't launch Spark until after I have populated all the data onto HDFS but it makes sense to mention the launch commands here. They are as follows:\n",
    ">```shell\n",
    "sudo /opt/spark/sbin/start-master.sh\n",
    "#sudo /opt/spark/sbin/stop-master.sh\n",
    "sudo /opt/spark/sbin/start-slaves.sh\n",
    "#sudo /opt/spark/sbin/stop-slaves.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Issue\n",
    "Failing to spark-sql\n",
    ">```shell\n",
    "spark-sql \\\n",
    "    --master spark://r1:7077 \\\n",
    "    --num-executors 1\n",
    "```\n",
    "*Failed to connect to master r1:7077*\n",
    "check the port status:\n",
    "```shell\n",
    "ssh -v -p 7077 r1\n",
    "```\n",
    "*OpenSSH_7.4p1 Raspbian-10+deb9u1, OpenSSL 1.0.2l  25 May 2017  \n",
    "debug1: Reading configuration data /etc/ssh/ssh_config  \n",
    "debug1: /etc/ssh/ssh_config line 19: Applying options for **  \n",
    "debug1: Connecting to r1 [192.168.1.108] port 7077.  \n",
    "debug1: connect to address 192.168.1.108 port 7077: Connection refused  \n",
    "ssh: connect to host r1 port 7077: Connection refused*  \n",
    "check port status:\n",
    "```shell\n",
    "netstat -pln\n",
    "```\n",
    "127.0.0.1:7077 is open, but 192.168.1.108:7077 is not open. \n",
    "redo this step, I just place r1 before, suppose the mechanism is: if there is only master server as slave server, Spark will only listen on 127.0.0.1:7077 for local scope. If there is other remote slave server in this configure file, than Spark will listern on 192.168.1.108:7077 for cluster scope.\n",
    "```shell\n",
    "sudo vi /opt/spark/conf/slaves\n",
    "r1\n",
    "r2\n",
    "r3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is some issue with Hive metastore when restart this platform for the first.\n",
    "*Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Hive Schema version 2.3.0 does not match metastore's schema version 1.2.0 Metastore is not upgraded or corrupt*\n",
    "\n",
    "doing the following steps to fix this issue:\n",
    "```shell\n",
    "cd /opt/hive/scripts/metastore/upgrade/mysql\n",
    "\n",
    "mysql --verbose\n",
    "\n",
    "use metastore;\n",
    "source upgrade-1.2.0-to-2.0.0.mysql.sql\n",
    "source upgrade-2.0.0-to-2.1.0.mysql.sql\n",
    "source upgrade-2.1.0-to-2.2.0.mysql.sql\n",
    "source upgrade-2.2.0-to-2.3.0.mysql.sql\n",
    "exit\n",
    "#schematool -dbType mysql -upgradeSchemaFrom 1.2.0\n",
    "```\n",
    "redo this step:\n",
    ">hive --service metastore &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Zookeeper\n",
    ">```shell\n",
    "#on client:  \n",
    "scp zookeeper-3.4.11.tar.gz pi@192.168.1.108:./  \n",
    "#on master server:\n",
    "sudo mkdir -p /opt/zookeeper\n",
    "sudo tar xvf zookeeper-3.4.11.tar.gz \\\n",
    "--directory=/opt/zookeeper \\\n",
    "--strip 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">configure zookeeper:\n",
    "```shell\n",
    "sudo vi $ZOO_HOME/conf/zoo.cfg\n",
    "```\n",
    "```\n",
    "# The number of milliseconds of each tick\n",
    "tickTime=5000\n",
    "# The number of ticks that the initial \n",
    "# synchronization phase can take\n",
    "initLimit=10\n",
    "# The number of ticks that can pass between \n",
    "# sending a request and getting an acknowledgement\n",
    "syncLimit=5\n",
    "# the directory where the snapshot is stored.\n",
    "# do not use /tmp for storage, /tmp here is just \n",
    "# example sakes.\n",
    "dataDir=/home/pi/zookeeper\n",
    "# the port at which the clients will connect\n",
    "clientPort=2181\n",
    "#\n",
    "# Be sure to read the maintenance section of the \n",
    "# administrator guide before turning on autopurge.\n",
    "#\n",
    "# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance\n",
    "#\n",
    "# The number of snapshots to retain in dataDir\n",
    "#autopurge.snapRetainCount=3\n",
    "# Purge task interval in hours\n",
    "# Set to \"0\" to disable auto purge feature\n",
    "#autopurge.purgeInterval=1\n",
    "initLimit=5\n",
    "syncLimit=2\n",
    "server.1=r1:2888:3888\n",
    "#server.2=r2:2888:3888\n",
    "#server.3=r3:2888:3888\n",
    "#This config is for three RaspberryPi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Start zookeeper by: \n",
    "```shell\n",
    "sudo /opt/zookeeper/bin/zkServer.sh start\n",
    "```\n",
    "```shell\n",
    "ZooKeeper JMX enabled by default\n",
    "Using config: /opt/zookeeper/bin/../conf/zoo.cfg\n",
    "Starting zookeeper ... STARTED\n",
    "```\n",
    "#### issue\n",
    ">check the status:\n",
    "```shell\n",
    "sudo /opt/zookeeper/bin/zkServer.sh status\n",
    "```\n",
    "```\n",
    "ZooKeeper JMX enabled by default\n",
    "Using config: /opt/zookeeper/bin/../conf/zoo.cfg\n",
    "Error contacting service. It is probably not running.\n",
    "```\n",
    "there is also error log:\n",
    "less /home/pi/zookeeper.out\n",
    ">>WARN  [main:QuorumPeer$QuorumServer@190] - Failed to resolve address: r3  \n",
    ">>java.net.UnknownHostException: r3: unknown error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">annotate the r2,r3 in the conf/zoo.cfg file, and restart, this time started successfully.\n",
    "```shell\n",
    "netstat -nlp | grep 2181\n",
    "```\n",
    "```\n",
    "tcp6       0      0 :::2181                 :::*                    LISTEN      -  \n",
    "```\n",
    "```shell\n",
    "sudo /opt/zookeeper/bin/zkServer.sh status\n",
    "```\n",
    "```ZooKeeper JMX enabled by default\n",
    "Using config: /opt/zookeeper/bin/../conf/zoo.cfg\n",
    "Mode: standalone\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Installing Kafka\n",
    ">```shell\n",
    "#on client:  \n",
    "scp kafka_2.12-1.0.0.tgz pi@192.168.1.108:./  \n",
    "#on master server:\n",
    "sudo mkdir -p /opt/kafka\n",
    "sudo tar xzvf kafka_2.12-1.0.0.tgz \\\n",
    "--directory=/opt/kafka \\\n",
    "--strip 1\n",
    "```\n",
    ">Config config/server.properties\n",
    "```shell\n",
    "sudo vi /opt/kafka/config/server.properties\n",
    "#The id of the broker. This must be set to a unique integer for each broker.\n",
    "broker.id=0 # increment this for each node (broker.id=1 on 2nd node etc.)\n",
    "#...\n",
    "#A comma seperated list of directories under which to store log files\n",
    "log.dirs=/opt/kafka/log # change to wherever you want the log to be\n",
    "#...\n",
    "#The minimum age of a log file to be eligible for deletion\n",
    "log.retention.hours=1 # change this based on your need\n",
    "#...\n",
    "#Zookeeper connection string (see zookeeper docs for details).\n",
    "#This is a comma separated host:port pairs, each corresponding to a zk\n",
    "#server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n",
    "#You can also append an optional chroot string to the urls to specify the\n",
    "#root directory for all kafka znodes.\n",
    "zookeeper.connect=r1:2181\n",
    "#zookeeper.connect=r1:2181,r2:2181,r3:2181\n",
    "```\n",
    ">Config bin/kafka-server-start.sh\n",
    "```shell\n",
    "sudo vi /opt/kafka/bin/kafka-server-start.sh\n",
    "```\n",
    "Add the following after all comments:\n",
    "```\n",
    "export JMX_PORT=${JMX_PORT:-9999}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For Raspberry Pi, if the memory on the Pi is small, the default settings (usually 1G) will have trouble starting JVM. Add the following:\n",
    "```shell\n",
    "export KAFKA_HEAP_OPTS=\"-Xmx256M -Xms128M\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Config bin/kafka-run-class.sh\n",
    "For Raspberry Pi OS: Java is running as -client instead of -server. However, default Kafka setting runs Java as -server. To change that:\n",
    "```shell\n",
    "sudo vi /opt/kafka/bin/kafka-run-class.sh\n",
    "```\n",
    "Find KAFKA_JVM_PERFORMANCE_OPTS and change to:\n",
    "```\n",
    "KAFKA_JVM_PERFORMANCE_OPTS=\"-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true\"\n",
    "###if 2.8 kafka:\n",
    "#KAFKA_JVM_PERFORMANCE_OPTS=\"-client -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Start Kafka on all nodes\n",
    "```shell\n",
    "sudo /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a topic\n",
    ">Let's create a topic named \"test\" with a single partition and only one replica:\n",
    "```shell\n",
    "$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper r1:2181 --replication-factor 1 --partitions 1 --topic test\n",
    "#We can now see that topic if we run the list topic command\n",
    "$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper r1:2181\n",
    "test\n",
    "```\n",
    "\n",
    "### Issue: \n",
    ">Error while executing topic command : Replication factor: 1 larger than available brokers: 0.\n",
    ">check broke id in zoo_keeper client:\n",
    "```shell\n",
    "$ZOO_HOME/bin/zkCli.sh\n",
    "ls /brokers/ids\n",
    "[]\n",
    "```\n",
    ">the broker is not connected to zoo_keeper  \n",
    ">stop Kafka server and restart, this failure resolved. Maybe some mistake keyboard operation last run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send some messages\n",
    ">Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default, each line will be sent as a separate message.\n",
    "Run the producer and then type a few messages into the console to send to the server.\n",
    "```shell\n",
    "$KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n",
    ">message2\n",
    ">message3\n",
    ">message4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a consumer\n",
    ">Kafka also has a command line consumer that will dump out messages to standard output.\n",
    "```shell\n",
    "$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning\n",
    "message2\n",
    "message3\n",
    "message4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Hbase\n",
    ">```shell\n",
    "#on client:\n",
    "scp hbase-1.3.1-bin.tar.gz pi@192.168.1.108:./\n",
    "#on master server:\n",
    "sudo mkdir -p /opt/hbase\n",
    "sudo tar xvf hbase-1.3.1-bin.tar.gz \\\n",
    "--directory=/opt/hbase \\\n",
    "--strip 1\n",
    "```\n",
    "### Installing HBase in Pseudo-Distributed Mode\n",
    "Let us now check how HBase is installed in pseudo-distributed mode.\n",
    "\n",
    ">CONFIGURING HBASE\n",
    "Before proceeding with HBase, configure Hadoop and HDFS on your local system or on a remote system and make sure they are running. Stop HBase if it is running.\n",
    ">Edit hbase-site.xml file to add the following properties.\n",
    "```shell\n",
    "sudo vi /opt/hbase/conf/hbase-site.xml\n",
    "```\n",
    "```xml\n",
    "<property>\n",
    "   <name>hbase.cluster.distributed</name>\n",
    "   <value>true</value>\n",
    "</property>\n",
    "```\n",
    ">It will mention in which mode HBase should be run. In the same file from the local file system, change the hbase.rootdir, your HDFS instance address, using the hdfs://// URI syntax. \n",
    "```xml\n",
    "<property>\n",
    "   <name>hbase.rootdir</name>\n",
    "   <value>hdfs://r1:9000/hbase</value>\n",
    "</property>\n",
    "```\n",
    "Starting HBase\n",
    "After configuration is over, browse to HBase home folder and start HBase using the following command.\n",
    "```shell\n",
    "sudo su\n",
    "/opt/hbase/bin/start-hbase.sh\n",
    "```\n",
    "Note: Before starting HBase, make sure Hadoop is running.\n",
    "\n",
    ">Checking the HBase Directory in HDFS\n",
    "HBase creates its directory in HDFS. To see the created directory, browse to Hadoop bin and type the following command.\n",
    "```shell\n",
    "hdfs dfs -ls /hbase\n",
    "```\n",
    "```\n",
    "Found 7 items\n",
    "drwxr-xr-x   - root supergroup          0 2017-11-12 05:23 /hbase/.tmp\n",
    "drwxr-xr-x   - root supergroup          0 2017-11-12 05:24 /hbase/MasterProcWALs\n",
    "drwxr-xr-x   - root supergroup          0 2017-11-12 05:23 /hbase/WALs\n",
    "drwxr-xr-x   - root supergroup          0 2017-11-12 05:23 /hbase/data\n",
    "-rw-r--r--   3 root supergroup         42 2017-11-12 05:23 /hbase/hbase.id\n",
    "-rw-r--r--   3 root supergroup          7 2017-11-12 05:23 /hbase/hbase.version\n",
    "drwxr-xr-x   - root supergroup          0 2017-11-12 05:23 /hbase/oldWALs\n",
    "```\n",
    "##### Starting HBaseShell\n",
    ">After Installing HBase successfully, you can start HBase Shell. Below given are the sequence of steps that are to be followed to start the HBase shell. Open the terminal, and login as super user.\n",
    "\n",
    ">Start Hadoop File System\n",
    "Browse through Hadoop home sbin folder and start Hadoop file system as shown below.\n",
    "```shell\n",
    "HADOOP_HOME/sbin/start-all.sh\n",
    "```\n",
    "Start HBase\n",
    "Browse through the HBase root directory bin folder and start HBase.\n",
    "```shell\n",
    "/usr/local/HBase/bin/start-hbase.sh\n",
    "```\n",
    "Connect to your running instance of HBase using the hbase shell command, located in the bin/ directory of your HBase install. In this example, some usage and version information that is printed when you start HBase Shell has been omitted. The HBase Shell prompt ends with a > character.\n",
    "```shell\n",
    "HBASE_HOME/bin/hbase shell\n",
    "```\n",
    "Create a table and populate it with data.\n",
    "\n",
    ">You can use the HBase Shell to create a table, populate it with data, scan and get values from it, using the same procedure as in [shell exercises](https://hbase.apache.org/book.html#shell_exercises).\n",
    "##### Start HBase Master Server\n",
    ">The HMaster server controls the HBase cluster. You can start up to 9 backup HMaster servers, which makes 10 total HMasters, counting the primary. To start a backup HMaster, use the local-master-backup.sh. For each backup master you want to start, add a parameter representing the port offset for that master. Each HMaster uses three ports (16010, 16020, and 16030 by default). The port offset is added to these ports, so using an offset of 2, the backup HMaster would use ports 16012, 16022, and 16032. The following command starts 3 backup servers using ports 16012/16022/16032, 16013/16023/16033, and 16015/16025/16035.\n",
    "```shell\n",
    "#HBASE_HOME/bin/local-master-backup.sh start 2 3 5\n",
    "HBASE_HOME/bin/local-master-backup.sh start 2\n",
    "```\n",
    "To kill a backup master without killing the entire cluster, you need to find its process ID (PID). The PID is stored in a file with a name like /tmp/hbase-USER-X-master.pid. The only contents of the file is the PID. You can use the kill -9 command to kill that PID. The following command will kill the master with port offset 1, but leave the cluster running:\n",
    "```shell\n",
    "cat /tmp/hbase-root-2-master.pid |xargs kill -9\n",
    "```\n",
    "##### Start and stop additional RegionServers\n",
    "\n",
    ">The HRegionServer manages the data in its StoreFiles as directed by the HMaster. Generally, one HRegionServer runs per node in the cluster. Running multiple HRegionServers on the same system can be useful for testing in pseudo-distributed mode. The local-regionservers.sh command allows you to run multiple RegionServers. It works in a similar way to the local-master-backup.sh command, in that each parameter you provide represents the port offset for an instance. Each RegionServer requires two ports, and the default ports are 16020 and 16030. However, the base ports for additional RegionServers are not the default ports since the default ports are used by the HMaster, which is also a RegionServer since HBase version 1.0.0. The base ports are 16200 and 16300 instead. You can run 99 additional RegionServers that are not a HMaster or backup HMaster, on a server. The following command starts four additional RegionServers, running on sequential ports starting at 16202/16302 (base ports 16200/16300 plus 2).\n",
    "```shell\n",
    "#HBASE_HOME/bin/local-regionservers.sh start 2 3 4 5\n",
    "HBASE_HOME/bin/local-regionservers.sh start 2\n",
    "```\n",
    "To stop a RegionServer manually, use the local-regionservers.sh command with the stop parameter and the offset of the server to stop.\n",
    "```shell\n",
    "HBASE_HOME/bin/local-regionservers.sh stop 2\n",
    "```\n",
    "##### HBase Web Interface\n",
    ">To access the web interface of HBase, type the following url in the browser.\n",
    "[http://192.168.1.108:60010](http://192.168.1.108:60010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Pig\n",
    ">```shell\n",
    "#on client:\n",
    "scp pig-0.17.0.tar.gz pi@192.168.1.108:./\n",
    "#on master server:\n",
    "sudo mkdir -p /opt/pig\n",
    "sudo tar xvf pig-0.17.0.tar.gz \\\n",
    "--directory=/opt/pig \\\n",
    "--strip 1\n",
    "```\n",
    ">In the conf folder of Pig, we have a file named pig.properties. In the pig.properties file, you can set various parameters as given below.\n",
    "```shell\n",
    "pig -h properties\n",
    "```\n",
    "**Mapreduce Mode** - To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation. Mapreduce mode is the default mode; you can, but don't need to, specify it using the -x flag (pig OR pig -x mapreduce).  \n",
    "**Spark Mode** - To run Pig in Spark mode, you need access to a Spark, Yarn or Mesos cluster and HDFS installation. Specify Spark mode using the -x flag (-x spark). In Spark execution mode, it is necessary to set env::SPARK_MASTER to an appropriate value (local - local mode, yarn-client - yarn-client mode, mesos://host:port - spark on mesos or spark://host:port - spark cluster. For more information refer to spark documentation on Master URLs, yarn-cluster mode is currently not supported). Pig scripts run on Spark can take advantage of the dynamic allocation feature. The feature can be enabled by simply enabling spark.dynamicAllocation.enabled. Refer to spark configuration for additional configuration details. In general all properties in the pig script prefixed with spark. are copied to the Spark Application Configuration. Please note that Yarn auxillary service need to be enabled on Spark for this to work. See Spark documentation for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restart steps\n",
    ">```shell\n",
    "sudo su\n",
    "start-dfs.sh\n",
    "hive --service metastore &\n",
    "/opt/spark/sbin/start-master.sh\n",
    "/opt/spark/sbin/start-slaves.sh\n",
    "/opt/zookeeper/bin/zkServer.sh start\n",
    "/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop steps\n",
    ">```shell\n",
    "sudo su\n",
    "$HBASE_HOME/bin/stop-hbase.sh\n",
    "$HDFS_HOME/bin/stop-dfs.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
