{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Spark by Remote Jupyter Notebook via \"sparkmagic\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install sparkmagic and livy\n",
    "[The \"sparkmagic\" project on github](https://github.com/jupyter-incubator/sparkmagic)  \n",
    "[Get started with Livy](https://livy.incubator.apache.org/get-started/)  \n",
    "The architecture diagram:\n",
    "<img src=\"diagram.png\" width=\"600\" img/>\n",
    "Install \"sparkmagic\" on remote, my case is on my Macbook:\n",
    "```shell\n",
    "conda install sparkmagic\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension \n",
    "jupyter serverextension enable --py sparkmagic\n",
    "```\n",
    "Start Livy service on Raspberry Pi master server, my case is r1 (192.168.31.108):  \n",
    "```shell\n",
    "sudo su\n",
    "#mkdir -p /opt/livy/livy-0.4.0-incubating-bin/logs\n",
    "cd /opt/livy/livy-0.4.0-incubating-bin/bin\n",
    "./livy-server\n",
    "```\n",
    "Livy UI is default on port 8998, my case is:http://r1:8998/ui  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Follow [this page](https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Magics%20in%20IPython%20Kernel.ipynb) to test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08ce407f758473fa0a025b1ce6bdcfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://raspberrypi1:8998/\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of numbers is 1 and its description is:\n",
      "(1) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480 []"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "print('First element of numbers is {} and its description is:\\n{}'.format(numbers.first(), numbers.toDebugString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "17/11/28 20:26:02 INFO driver.RSCDriver: Connecting to: 192.168.1.108:10000\n",
      "17/11/28 20:26:02 INFO driver.RSCDriver: Starting RPC server...\n",
      "17/11/28 20:26:04 INFO rpc.RpcServer: Connected to the port 10001\n",
      "17/11/28 20:26:04 WARN rsc.RSCConf: Your hostname, raspberrypi1, resolves to a loopback address; using 192.168.1.108  instead (on interface wlan0)\n",
      "17/11/28 20:26:04 WARN rsc.RSCConf: Set 'livy.rsc.rpc.server.address' if you need to bind to another address.\n",
      "17/11/28 20:26:06 INFO driver.RSCDriver: Received job request e2cff957-f4e1-4f88-942f-298f54d9164f\n",
      "17/11/28 20:26:06 INFO driver.RSCDriver: SparkContext not yet up, queueing job request.\n",
      "17/11/28 20:26:07 INFO spark.SparkContext: Running Spark version 2.2.0\n",
      "17/11/28 20:26:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/11/28 20:26:10 WARN util.Utils: Your hostname, raspberrypi1 resolves to a loopback address: 127.0.1.1; using 192.168.1.108 instead (on interface wlan0)\n",
      "17/11/28 20:26:10 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "17/11/28 20:26:10 INFO spark.SparkContext: Submitted application: livy-session-0\n",
      "17/11/28 20:26:10 INFO spark.SecurityManager: Changing view acls to: root\n",
      "17/11/28 20:26:10 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "17/11/28 20:26:10 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "17/11/28 20:26:10 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "17/11/28 20:26:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "17/11/28 20:26:11 INFO util.Utils: Successfully started service 'sparkDriver' on port 39137.\n",
      "17/11/28 20:26:11 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "17/11/28 20:26:11 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "17/11/28 20:26:11 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "17/11/28 20:26:11 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "17/11/28 20:26:11 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-79e1d971-8131-42b2-8b67-ed2b801687ae\n",
      "17/11/28 20:26:11 INFO memory.MemoryStore: MemoryStore started with capacity 197.0 MB\n",
      "17/11/28 20:26:11 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "17/11/28 20:26:12 INFO util.log: Logging initialized @13724ms\n",
      "17/11/28 20:26:12 INFO server.Server: jetty-9.3.z-SNAPSHOT\n",
      "17/11/28 20:26:12 INFO server.Server: Started @14190ms\n",
      "17/11/28 20:26:13 INFO server.AbstractConnector: Started ServerConnector@71a17f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "17/11/28 20:26:13 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5db505{/jobs,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ec33d5{/jobs/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19afcfc{/jobs/job,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16bead6{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ffee59{/stages,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14225f7{/stages/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17bfc3c{/stages/stage,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d37718{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a66a15{/stages/pool,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1673884{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11437a{/storage,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1990dba{/storage/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b8e6c9{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@167b5f5{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fed3e9{/environment,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@160be68{/environment/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11c6364{/executors,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11efa4e{/executors/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1211582{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16a999{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bff033{/static,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@99f3cb{/,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@256c80{/api,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e921{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@413d57{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:26:13 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.108:4040\n",
      "17/11/28 20:26:13 INFO spark.SparkContext: Added JAR file:/opt/livy/livy-0.4.0-incubating-bin/rsc-jars/livy-rsc-0.4.0-incubating.jar at spark://192.168.1.108:39137/jars/livy-rsc-0.4.0-incubating.jar with timestamp 1511871973429\n",
      "17/11/28 20:26:13 INFO spark.SparkContext: Added JAR file:/opt/livy/livy-0.4.0-incubating-bin/rsc-jars/netty-all-4.0.29.Final.jar at spark://192.168.1.108:39137/jars/netty-all-4.0.29.Final.jar with timestamp 1511871973433\n",
      "17/11/28 20:26:13 INFO spark.SparkContext: Added JAR file:/opt/livy/livy-0.4.0-incubating-bin/rsc-jars/livy-api-0.4.0-incubating.jar at spark://192.168.1.108:39137/jars/livy-api-0.4.0-incubating.jar with timestamp 1511871973434\n",
      "17/11/28 20:26:13 INFO spark.SparkContext: Added JAR file:/opt/livy/livy-0.4.0-incubating-bin/repl_2.11-jars/livy-core_2.11-0.4.0-incubating.jar at spark://192.168.1.108:39137/jars/livy-core_2.11-0.4.0-incubating.jar with timestamp 1511871973434\n",
      "17/11/28 20:26:13 INFO spark.SparkContext: Added JAR file:/opt/livy/livy-0.4.0-incubating-bin/repl_2.11-jars/commons-codec-1.9.jar at spark://192.168.1.108:39137/jars/commons-codec-1.9.jar with timestamp 1511871973435\n",
      "17/11/28 20:26:13 INFO spark.SparkContext: Added JAR file:/opt/livy/livy-0.4.0-incubating-bin/repl_2.11-jars/livy-repl_2.11-0.4.0-incubating.jar at spark://192.168.1.108:39137/jars/livy-repl_2.11-0.4.0-incubating.jar with timestamp 1511871973436\n",
      "17/11/28 20:26:13 INFO executor.Executor: Starting executor ID driver on host localhost\n",
      "17/11/28 20:26:13 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32937.\n",
      "17/11/28 20:26:13 INFO netty.NettyBlockTransferService: Server created on 192.168.1.108:32937\n",
      "17/11/28 20:26:13 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "17/11/28 20:26:13 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.108, 32937, None)\n",
      "17/11/28 20:26:13 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.1.108:32937 with 197.0 MB RAM, BlockManagerId(driver, 192.168.1.108, 32937, None)\n",
      "17/11/28 20:26:13 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.108, 32937, None)\n",
      "17/11/28 20:26:13 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.108, 32937, None)\n",
      "17/11/28 20:26:15 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@940e5d{/metrics/json,null,AVAILABLE,@Spark}\n",
      "17/11/28 20:27:36 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:446\n",
      "17/11/28 20:27:36 INFO scheduler.DAGScheduler: Got job 0 (runJob at PythonRDD.scala:446) with 1 output partitions\n",
      "17/11/28 20:27:36 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:446)\n",
      "17/11/28 20:27:36 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "17/11/28 20:27:36 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "17/11/28 20:27:36 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at RDD at PythonRDD.scala:48), which has no missing parents\n",
      "17/11/28 20:27:36 WARN util.SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes\n",
      "17/11/28 20:27:36 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.6 KB, free 197.0 MB)\n",
      "17/11/28 20:27:37 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.5 KB, free 197.0 MB)\n",
      "17/11/28 20:27:37 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.108:32937 (size: 2.5 KB, free: 197.0 MB)\n",
      "17/11/28 20:27:37 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006\n",
      "17/11/28 20:27:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[1] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0))\n",
      "17/11/28 20:27:37 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "17/11/28 20:27:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4837 bytes)\n",
      "17/11/28 20:27:37 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "17/11/28 20:27:37 INFO executor.Executor: Fetching spark://192.168.1.108:39137/jars/livy-core_2.11-0.4.0-incubating.jar with timestamp 1511871973434\n",
      "17/11/28 20:27:37 INFO client.TransportClientFactory: Successfully created connection to /192.168.1.108:39137 after 33 ms (0 ms spent in bootstraps)\n",
      "17/11/28 20:27:37 INFO util.Utils: Fetching spark://192.168.1.108:39137/jars/livy-core_2.11-0.4.0-incubating.jar to /tmp/spark-8df4a181-c949-476a-866a-32d6dfe7bac9/userFiles-a1d2a2c8-000e-4e78-ac62-b879d6c52d70/fetchFileTemp7175231393793129083.tmp\n",
      "17/11/28 20:27:38 INFO executor.Executor: Adding file:/tmp/spark-8df4a181-c949-476a-866a-32d6dfe7bac9/userFiles-a1d2a2c8-000e-4e78-ac62-b879d6c52d70/livy-core_2.11-0.4.0-incubating.jar to class loader\n",
      "17/11/28 20:27:38 INFO executor.Executor: Fetching spark://192.168.1.108:39137/jars/commons-codec-1.9.jar with timestamp 1511871973435\n",
      "17/11/28 20:27:38 INFO util.Utils: Fetching spark://192.168.1.108:39137/jars/commons-codec-1.9.jar to /tmp/spark-8df4a181-c949-476a-866a-32d6dfe7bac9/userFiles-a1d2a2c8-000e-4e78-ac62-b879d6c52d70/fetchFileTemp5891395398977655674.tmp\n",
      "17/11/28 20:27:38 INFO executor.Executor: Adding file:/tmp/spark-8df4a181-c949-476a-866a-32d6dfe7bac9/userFiles-a1d2a2c8-000e-4e78-ac62-b879d6c52d70/commons-codec-1.9.jar to class loader\n",
      "17/11/28 20:27:38 INFO executor.Executor: Fetching spark://192.168.1.108:39137/jars/livy-api-0.4.0-incubating.jar with timestamp 1511871973434\n",
      "17/11/28 20:27:38 INFO util.Utils: Fetching spark://192.168.1.108:39137/jars/livy-api-0.4.0-incubating.jar to /tmp/spark-8df4a181-c949-476a-866a-32d6dfe7bac9/userFiles-a1d2a2c8-000e-4e78-ac62-b879d6c52d70/fetchFileTemp3926005634995747385.tmp\n",
      "17/11/28 20:27:38 INFO executor.Executor: Adding file:/tmp/spark-8df4a181-c949-476a-866a-32d6dfe7bac9/userFiles-a1d2a2c8-000e-4e78-ac62-b879d6c52d70/livy-api-0.4.0-incubating.jar to class loader\n",
      "17/11/28 20:27:38 INFO executor.Executor: Fetching spark://192.168.1.108:39137/jars/netty-all-4.0.29.Final.jar with timestamp 1511871973433\n",
      "17/11/28 20:27:38 INFO util.Utils: Fetching spark://192.168.1.108:39137/jars/netty-all-4.0.29.Final.jar to /tmp/spark-8df4a181-c949-476a-866a-32d6dfe7bac9/userFiles-a1d2a2c8-000e-4e78-ac62-b879d6c52d70/fetchFileTemp6591632046167960217.tmp"
     ]
    }
   ],
   "source": [
    "%spark logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "u'Table or view not found: hivesampletable; line 1 pos 14'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 556, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "AnalysisException: u'Table or view not found: hivesampletable; line 1 pos 14'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark -c sql -o df_hvac --maxrows 10\n",
    "SELECT * FROM hivesampletable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hvac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abba92634ebc47409be1202921b73053"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://raspberrypi1:8998/ui\n"
     ]
    },
    {
     "ename": "HttpClientException",
     "evalue": "Invalid status code '200' from http://raspberrypi1:8998/ui/sessions with error payload: <html>\n      <head>\n      <link rel=\"stylesheet\" href=\"/static/css/bootstrap.min.css\" type=\"text/css\"/>\n      <link rel=\"stylesheet\" href=\"/static/css/dataTables.bootstrap.min.css\" type=\"text/css\"/>\n      <link rel=\"stylesheet\" href=\"/static/css/livy-ui.css\" type=\"text/css\"/>\n      <script src=\"/static/js/jquery-3.2.1.min.js\"></script>\n      <script src=\"/static/js/bootstrap.min.js\"></script>\n      <script src=\"/static/js/jquery.dataTables.min.js\"></script>\n      <script src=\"/static/js/dataTables.bootstrap.min.js\"></script>\n      <script src=\"/static/js/livy-ui.js\"></script>\n      <title>Livy - 404</title>\n    </head>\n      <body>\n        <div class=\"container-fluid\">\n          <nav class=\"navbar navbar-default\">\n      <div class=\"container-fluid\">\n        <div class=\"navbar-header\">\n          <a class=\"navbar-brand\" href=\"/ui\">\n            <img alt=\"Livy\" src=\"/static/img/livy-mini-logo.png\"/>\n          </a>\n        </div>\n        <div class=\"collapse navbar-collapse\">\n          <ul class=\"nav navbar-nav\">\n            \n          </ul>\n        </div>\n      </div>\n    </nav>\n          <h3>404 No Such Page</h3>\n        </div>\n      </body>\n    </html>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpClientException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/hdijupyterutils/ipywidgetfactory.py\u001b[0m in \u001b[0;36msubmit_clicked\u001b[0;34m(self, button)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msubmit_clicked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbutton\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_widget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/sparkmagic/controllerwidget/createsessionwidget.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             self.ipython_display.send_error(\"\"\"Could not add session with\n",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36madd_session\u001b[0;34m(self, name, endpoint, skip_if_exists, properties)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_livy_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_session_id_for_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/sparkmagic/livyclientlib/livyreliablehttpclient.py\u001b[0m in \u001b[0;36mpost_session\u001b[0;34m(self, properties)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/sessions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, relative_url, accepted_status_codes, data)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;34m\"\"\"Sends a post request. Returns a response.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, relative_url, accepted_status_codes, function, data)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/lib/python3.6/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py\u001b[0m in \u001b[0;36m_send_request_helper\u001b[0;34m(self, url, accepted_status_codes, function, data, retry_count)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     raise HttpClientException(u\"Invalid status code '{}' from {} with error payload: {}\"\n\u001b[0;32m---> 95\u001b[0;31m                                               .format(status, url, text))\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpClientException\u001b[0m: Invalid status code '200' from http://raspberrypi1:8998/ui/sessions with error payload: <html>\n      <head>\n      <link rel=\"stylesheet\" href=\"/static/css/bootstrap.min.css\" type=\"text/css\"/>\n      <link rel=\"stylesheet\" href=\"/static/css/dataTables.bootstrap.min.css\" type=\"text/css\"/>\n      <link rel=\"stylesheet\" href=\"/static/css/livy-ui.css\" type=\"text/css\"/>\n      <script src=\"/static/js/jquery-3.2.1.min.js\"></script>\n      <script src=\"/static/js/bootstrap.min.js\"></script>\n      <script src=\"/static/js/jquery.dataTables.min.js\"></script>\n      <script src=\"/static/js/dataTables.bootstrap.min.js\"></script>\n      <script src=\"/static/js/livy-ui.js\"></script>\n      <title>Livy - 404</title>\n    </head>\n      <body>\n        <div class=\"container-fluid\">\n          <nav class=\"navbar navbar-default\">\n      <div class=\"container-fluid\">\n        <div class=\"navbar-header\">\n          <a class=\"navbar-brand\" href=\"/ui\">\n            <img alt=\"Livy\" src=\"/static/img/livy-mini-logo.png\"/>\n          </a>\n        </div>\n        <div class=\"collapse navbar-collapse\">\n          <ul class=\"nav navbar-nav\">\n            \n          </ul>\n        </div>\n      </div>\n    </nav>\n          <h3>404 No Such Page</h3>\n        </div>\n      </body>\n    </html>"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://raspberrypi1:8998\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Int = 1"
     ]
    }
   ],
   "source": [
    "%%spark -s test_scala\n",
    "val hvacText = sc.parallelize(Array(1, 2, 3, 4))\n",
    "hvacText.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -s test_scala -c sql -o my_df_from_scala --maxrows 10\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
